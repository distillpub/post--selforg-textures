<d-article>
{% include contents.html %}
{% include nextPrev.html %}

<p>NCAs (Neural Cellular Automata) have been shown to be capable of a diverse set of learned behaviours: from generating stable, regenerating, static images <d-cite key="Mordvintsev_Randazzo_Niklasson_Levin_2020"></d-cite>, to segmenting images <d-cite key="sandler"></d-cite>, and learning to "self-classify" shapes <d-cite key="Randazzo_Mordvintsev_Niklasson_Levin_Greydanus_2020"></d-cite>. The inductive prior imposed by using cellular automata is powerful - a system of individual agents running the same learned local rule can solve surprisingly complex tasks, requiring coordination between cells across larger distances. By necessity, they solve these tasks in a massively parallel and inherently degenerate<d-footnote> Degenerate in this case refers to the biological concept of degeneracy.</d-footnote> way - each cell must be able to take on the role of any other cell - as a result they tend to generalize well to unseen situations. We make the case that the cells learn distributed, local, algorithms. </p>

<p>In this work we set out to qualitatively explore how, and what kind of behaviours, a CA will learn. We task the NCA with a loss allowing for a degree of ambiguity and investigate the subsequent learned behaviour. To do this, we employ an old trick: differentiable parameterizations <d-cite key="Mordvintsev2018-dc"></d-cite>.</p>

<h2 id='patterns-textures'>Patterns, textures and physical processes</h2>
<p></p>
<figure>
<img src='images/zebra.jpg' style='width: 450px'>
<figcaption>A pair of Zebra. Zebra are said to have unique stripes.</figcaption>
</figure>

<p>Zebra stripes consist of a characteristic pattern. Ask almost anyone to identify zebra stripes in a set of images, and they&#39;ll have no trouble doing so. Ask them to describe what zebra stripes look like, and they will gladly tell you that they are stripes of varying shape and direction, alternating in black and white. At the same time, it is said that no two zebra have identical stripes<d-footnote> Perhaps apocryphal, but we would wager it true were you to pedantically measure every molecule on the surface of said zebra. Point is - "zebra stripes" as a concept in human understanding refers to the general structure of a black and white striped pattern, and not to a specific mapping from location to colour.</d-footnote>. One can then draw the conclusion that evolution has programmed the cells responsible for creating the zebra pattern to generate a pattern of a certain quality, with certain characteristics, as opposed to programming them with the blueprints for an exact bitmap of the edges and locations of stripes to be moulded to the surface of the Zebra's body.</p>

<p>Patterns and textures are ill-defined concepts. The Cambridge English Dictionary defines a pattern as &quot;any regularly repeated arrangement, especially a design made from repeated lines, shapes, or colours on a surface&quot;. This is all good and well, but this pseudo definition falls apart rather quickly when looking at patterns and textures that impart a feeling or quality, rather than a specific repeating property. A coloured fuzzy rug, for instance, can be considered a pattern or a texture, but is composed of strands pointing in random directions with small random variations in size and color, and thus no discernable regularity to the pattern. The famous Penrose tilings do not repeat (insofar as that they are not translationally invariant), but show them to anyone and they&#39;ll describe them as a pattern or a texture. Most patterns in nature are outputs of locally interacting processes that may or may not be stochastic in nature, but are often based upon fairly simple rules. There has been extensive work studying models which can give rise to such patterns in nature, with most work based on Turing&#39;s seminal paper on morphogenesis. <d-cite key="Turing_1990"></d-cite></p>

<p>As a result, when having any model learn to produce textures or patterns we want the model to learn a generation process for the pattern or a method to sample from the distribution governing this pattern. The first hurdle is to choose an appropriate loss function, or qualitative measure of quality of the recreation of the pattern. To do so, we employ the ideas from Differentiable Parametrization. We employ the building-block aspect of differentiable models to use the Cellular Automata as the parametrization layer for an image we are trying to &quot;stylize&quot;. In this case, instead of having an image be re-styled in a new way, our starting point is completely unconstrained - it&#39;s the raw output of an NCA with random weights. We effectively use the NCAs as the "renderer", and some pre-trained differentiable model as a distinguisher of patterns - providing the gradient necessary for the renderer to learn to produce a pattern of a certain style.</p>
<h3>From Turing to Cellular Automata to Neural Networks</h3>
<p>NCAs are well suited to generating textures. We outline our reasoning and train of thought as to why we apply them to texture generation - starting with existing knowledge of texture generation in nature and demonstrating how one may arrive at the conclusion that NCAs are a very reasonable model for learning texture generation.</p>

<p>PDEs</p>
<p>In &quot;The Chemical Basis of Morphogenesis&quot; <d-cite key="Turing-1990"></d-cite>, Alan Turing proposed that simple physical processes of reaction and diffusion, easily modelled by partial differential equations, lie behind pattern formation in nature (such as the aforementioned Zebra stripes). Extensive work has since been done to identify PDEs modeling reaction-diffusion and evaluating their behaviour. One of the more celebrated examples is the Gray-Scott model of reaction diffusion <d-cite key="gray-scott"></d-cite>. This process has given us a veritable zoo of interesting behaviour, explorable by simply tuning the two parameters of the system. We strongly encourage readers to visit this excellent <a href='http://mrob.com/pub/comp/xmorphia/'>interactive atlas</a> of the different regions of the Gray-Scott reaction diffusion model 	to truly get a sense for the extreme variety of behaviour that&#39;s hidden behind two simple knobs. </p>

<p>A somewhat more general version of the above systems can be described by a simple Partial Differential Equation (PDE) over the state space of an image. </p>

<p>$\frac{\partial \mathbf{s} }{\partial  t } &#x3D; f(\textbf{s}, \nabla_\mathbf{x} \textbf{s}, \nabla_\mathbf{x}^{2}\textbf{s})$</p>

<p>Here, $f()$ is simply a function that depends on the gradient and laplacian of the state space and determines the time evolution of this state space. In less mathematical terms - we&#39;ve defined a system where every point of the image changes with time, in a way that depends on how the image currently changes in space. </p>
<h4>To (N)CAs</h4>

<p>The preferred way to evolve differential equations, especially for those less mathematically inclined, is to throw them, at first sight, into a numerical differential equation solver. Indeed, this is sometimes the <strong>only</strong> way, as many PDEs and ODEs of interest don&#39;t have closed form solutions (often even deceptively simple ones, such as the <a href='https://en.wikipedia.org/wiki/Three-body_problem'>three-body problem</a>). Numerically solving PDEs and ODEs is a vast and well established field, however, the biggest and most important hammer in the metaphorical toolkit is establishing some regimen of discretizing the space of the ODE (or PDE). In the case of ODEs in time, taking steps of fixed or varying size makes sense. In our case, the logical approach is to discretize the continuous 2D image space into a 2D raster grid. Boundary conditions are, of course, to worry about, but we can solve this by moving to a toroidal world - where each dimension simply wraps around. </p>

<p>Our next task is to discretize the gradient and laplacian operators - for this we use the <a href='https://en.wikipedia.org/wiki/Sobel_operator'>sobel operator</a> and the <a href='https://en.wikipedia.org/wiki/Discrete_Laplace_operator'>9-point variant</a> of the discrete laplace operator. </p>

<p>We now have a space-discretized version of our PDE that looks very much like a Cellular Automata - the time evolution of each discrete point in the raster grid depends only on its immediate neighbours.</p>

<h4>To Neural Networks</h4>
<p>The final step to solving or implementing the above general PDE for texture generation is to translate it to the &quot;language&quot; of deep learning. All the operations involved in iteratively evaluating the generalized PDE exist as commonly used operations in most deep learning frameworks. </p>
<h3>NCAs as pattern generators</h3>

<p></p>
<figure>
<img src='images/texture_model.svg' style='width: 450px'>
<figcaption>Texture NCA model.</figcaption>
</figure>


<p>We build on the Growing CA NCA model <d-cite key="growing-ca"></d-cite>, complete with built-in quantization of weights, and the batch pool mechanism to approximate long-term training. </p>
<h4>Loss function: </h4>
<p>Differentiable Parametrizations <d-cite key="Mordvintsev_Pezzotti_Schubert_Olah_2018"></d-cite> explains why VGG is the most commonly used network for implementing style-transfer. We use VGG for the same reason. We start with a template image, $\vec{x}$, we feed into VGG. We then collect statistics from certain layers (<i>block{1...6}_conv1, block4_conv2</i>) in the form of the raw activation values of the neurons in these layers. Finally, we feed the output, after between 32 and 64 iterations, of our Neural CA into VGG. Our loss is simply the $L_2$ distance between the activations of these neurons with the NCA input and their activations with the template image. We keep the weights of VGG frozen, and use ADAM <d-cite key="adam"></d-cite> to update the weights of the NCA.</p>
<h4>Dataset: </h4>
<p>The template images for this dataset are from the Oxford Describable Textures Dataset <d-cite key="dtd"></d-cite>.  The aim of this dataset was to provide a training and testing set for algorithms to recognize textures and to describe textures using words. The textures were collected to match 47 &quot;attributes&quot; - such as &quot;bumpy&quot; or &quot;polka-dotted&quot;. These 47 attributes were in turn distilled from a set of common words used to describe textures identified by Bhusan, Rao and Lohse <d-cite key="bhusan"></d-cite>. </p>
<h4>Results:</h4>
<p>After a few iterations of training with the above picture as a template, we see the NCA converges to a solution. The very first thing to notice is that the resulting solution found in the NCA parameters is <strong>not</strong> time-invariant - in other words - it&#39;s constantly changing! </p>

<p>This is not completely unexpected. In <i>Differentiable Parametrizations</i>, the authors noted that the images produced when backpropagating into the images space would end up different every time the algorithm was run, due to the stochastic nature of the parametrizations. In our model, we find that we attain such alignment in the temporal dimension, without optimizing for it - a welcome surprise. We believe the reason is twofold. First, we apply our loss after a random number of iterations of the NCA - this means that at any given time step the pattern must be in a state that minimizes the loss. Secondly, the stochastic updates, local communication and quantization all limit and regularize the magnitude of updates at each iteration, meaning the absolute difference between one iteration and the next is often not drastic. We hypothesize that these two properties encourage the NCA to find a solution where each iteration is <strong>aligned</strong> with the previous iteration and thus the motion, or appearance of motion, we see in the trained NCA is the NCA traversing this manifold of locally aligned solutions. </p>

<p>We believe that finding temporally aligned solutions is equivalent to finding an algorithm, or process, that generates the template pattern. Below we demonstrate some exciting behaviours observed when training the NCA on different template images.  </p>

<p></p>
<figure>
<video src='videos/grid.mov' autoplay loop muted style='width: 450px'></video>
<figcaption>An NCA trained to create a pattern in the style of <b>chequered_0121.jpg</b>.</figcaption>
</figure>

<p>Here, we see that the NCA is trained using a template image of a simple black and white grid. </p>
<p>We notice that: </p>

<ul><li>Initially, a non-aligned grid of black and white quadrilaterals is formed. </li>
<li>As time progresses, the quadrilaterals seemingly grow or shrink in both $\vec{x}$ and $\vec{y}$ to more closely approximate squares. Quadrilaterals of both colours either emerge or disappear. Both of these behaviours seem to be an attempt to find local consistency. </li>
<li>After a longer time, the grid often ends up in perfect consistency.</li></ul>

<p>Such behaviour is not entirely unlike what one would expect in a hand-engineered algorithm to produce a consistent grid with local communication.</p>

<p></p>
<figure>
<video src='videos/bubbles.mov' autoplay loop muted style='width: 450px'></video>
<figcaption>An NCA trained to create a pattern in the style of <b>bubbly0101.jpg</b>.</figcaption>
</figure>

<p>In this video, the NCA has learned to reproduce a texture based off a template of clear bubbles on a blue background. One of the most interesting behaviours we observe is that the density of bubbles remains fairly constant. If we re-initialize the grid states, or interactively destroy states, we see a multitude of bubbles &quot;forming&quot;. However - as soon as two bubbles get too close to each other, one of them will spontaneously collapse and disappear, ensuring a constant density. We regard these bubbles as &quot;<a href='#solitons'>solitons</a>&quot; in the solution space of our NCA - a concept we will discuss and investigate at length in a bit.</p>

<p>If we speed the animation up, we see that different bubbles move at different speeds, yet they never collide or touch each other. Simultaneously, they are self-correcting - a damaged bubble will re-grow.</p>

<p>This behaviour is remarkable because it&#39;s behaviour that&#39;s spontaneously arisen without any external loss or auxiliary loss. All of these properties are learned from a combination of the template image, the information stored in the layers of VGG and the inductive bias of the NCA. The NCA has found a rule that effectively approximates many of the properties we expect of bubbles we see in this image - effectively it has learned a process that generates this pattern of bubbles in a robust and &quot;realistic&quot; way.</p>

<p></p>
<figure>
<video src='videos/viking.mov' autoplay loop muted style='width: 450px'></video>
<figcaption>An NCA trained to create a pattern in the style of <b>interlaced_0172.jpg</b>.</figcaption>
</figure>

<p>Here we see one of the authors&#39; favourite patterns - a simple geometric &quot;weave&quot;. Again, we notice the NCA seems to have learned an algorithm for producing this pattern. Each &quot;thread&quot; alternatively joins or detaches from other threads in order to produce the final pattern. This is strikingly similar to what one would attempt to implement in the case of being asked to programmatically generate the above pattern - some algorithm to stochastically weave threads together.</p>



<figure>
<video src='videos/lines.mov' autoplay loop muted style='width: 450px'></video>
<figcaption>An NCA trained to create a pattern in the style of <b>banded_0037.jpg</b>.</figcaption>
</figure>

<p>This NCA also exhibits an algorithm for solving consistency in a striped pattern. The seams in ill-fitting stripes travel up, or down, the stripe, until either they merge to form a single straight stripe, or a stripe shrinks and disappears. Were this to be implemented algorithmically with local communication, it&#39;s not infeasible that a similar algorithm for finding consistency among the stripes would be used.</p>
<h3>Related work </h3>
<p>Pattern generation is by no means a novel field. There has been extensive work predating deep-learning. As briefly touched upon, the inspiration for much of the work came from Turing&#39;s work on pattern generation through local interaction, and later papers based on this principle. However, we also wish to acknowledge some works that we feel have kinship with ours. </p>
<h4>Interactive Evolution of Camouflage</h4>
<p>In Interactive Evolution of Camouflage <d-cite key="ieoc_reynolds"></d-cite>, Craig Reynolds uses a texture description language, consisting of generators and operators, to parametrize a texture patch, which is presented to human viewers who have to decide which patches are the worst at &quot;camouflaging&quot; themselves against a chosen background texture. The population is updated in an evolutionary fashion to maximize &quot;camouflage&quot;, resulting in a texture exhibiting the most camouflage (to human eyes) after a number of iterations. We see strong parallels to our work - instead of a texture generation language, we have an NCA parametrize the texture, and instead of human reviewers we use VGG as an evaluator of the quality of a generated pattern. We believe a fundamental difference lies in the solution space of an NCA. A texture generation language comes with a number of inductive biases and learns a deterministic mapping from coordinate to colour. Our method appears to learn more general algorithms and behaviours giving rise to the target pattern.</p>

<h2 id='feature-visualization'>Feature Visualization</h2>

<p></p>
<figure>
<img src='images/butterfly_eye.jpg' style='width: 450px'></img>
<figcaption>A butterfly with an "eye-spot" on the wings.</figcaption>
</figure>

<p>We&#39;ve now explored some of the fascinating behaviours learned by the NCA when presented with a template image. What if we want to see them learn even more &quot;unconstrained&quot; behaviour? </p>

<p>Some butterflies have remarkably lifelike eyes on their wings. It&#39;s unlikely the butterflies are even aware of this incredible artwork on their own bodies. Evolution placed these there to trigger a response of fear in potential predators. It is likely that neither the predator nor the butterfly has a concept for what an eye is or what an eye does, or even less so any <a href='https://en.wikipedia.org/wiki/Theory_of_mind'>theory of mind</a> regarding the consciousness of the other, but evolutionary development has nonetheless reached a state where the butterfly ingeniously tricks predators into fearing a harmless bug instead of consuming it. </p>

<p>Even more remarkable is the fact that the individual cells composing the butterfly's wings can self assemble into coherent, beautiful, shapes far larger than an individual cell - indeed a cell is on the order of $1^{-5}m$ <d-cite key="Ohno2015-uj"></d-cite> while the features on the wings will grow to as large as $1^{-3}m$ <d-cite key="Iwata2016-mw"></d-cite>. Communication over this distance implies a self-organization over a distance of hundreds or thousands of cells to generate a coherent image of an eye that evolved simply to act as a visual stimuli for an entirely different species. Of course, this pales in comparison to the morphogenesis that occurs in animal and plant bodies, where structures consisting of millions of cells will specialize and coordinate to generate the target morphology. </p>

<p>A common approach to investigating neural networks is to look at what inhibits or excites individual neurons in a network <d-cite key="openai-microscope"></d-cite>. Just as neuroscientists and biologists have often treated cells and cell structures and neurons as black-box models to be investigated, measured and reverse-engineered, there is a large contemporary body of work on doing the same with neural networks. </p>

<p>We can explore this idea with minimal effort by taking our VGG implementation from when we explored pattern generation, and exploring what happens if we task the NCA to enter a state that excites a given neuron in VGG. One of the common resulting NCAs we notice is eye and eye-related shapes - such as the video below - likely as a result of having to detect various animals in ImageNet. Much like the cells forming the eyes on the wings of butterflies to excite certain neurons in the brains of predators, our NCA has learned to have cells collaborate to produce a pattern exciting certain neurons in an arbitrary external neural network.</p>

<p></p>
<figure>
<video src='videos/eyes_vgg.mov' autoplay loop muted style='width: 450px'></video>
<figcaption>An NCA trained to excite <b>mixed4a_472</b> in VGG.</figcaption>
</figure>

<h3>Experiment: Feature Visualization</h3>
<h4>Model: </h4>
<p>We use a model identical to the one used for exploring pattern generation. </p>
<h4>Loss function: </h4>
<p>We use VGG, as before. Our loss maximizes the activations of the chosen neurons in VGG, when evaluated on the output of the NCA. We add an auxiliary loss to encourage the outputs of the NCA to be $\in [0,1]$, as this is not inherently built into the model. We keep the weights of VGG frozen, and use ADAM <d-cite key="adam"></d-cite> to update the weights of the NCA.</p>
<h4>Dataset: </h4>
<p>There is no explicit dataset for this task. VGG is trained on ImageNet. The layers and neurons we chose to excite are sampled qualitatively from OpenAI Microscope.</p>
<h4>Results:</h4>
<p>Similar to the pattern generation experiment, we see quick convergence and a tendency to find temporally dynamic solutions - i.e. most resulting NCAs don&#39;t stay still.</p>

<h3>Interesting behaviours of inception solitons</h3>
<p></p>
<figure>
<video src='videos/eyes.mov' autoplay loop muted style='width: 450px'></video>
<figcaption>An NCA trained to excite <b>mixed4c_439</b> in VGG.</figcaption>
</figure>
<p>Solitons in the form of regular circle-like shapes with internal structure are quite commonly observed in the inception renderings. Two solitons approaching each other too closely may cause one of both of them to decay. We also observe that solitons can "divide" into two complete new solitons.</p>
<p></p>
<figure>
<video src='videos/moving_thread.mov' autoplay loop muted style='width: 450px'></video>
<figcaption>An NCA trained to excite <b>mixed4b_454</b> in VGG.</figcaption>
</figure>
<p>In textures that are composed of threads or lines, or in certain excitations of VGG neurons where the resulting NCA has a &quot;thread-like&quot; quality, the threads grow in their respective directions and will join other threads, or grow around them, as required. This behaviour is similar to the regular lines observed in the striped patterns during pattern generation.</p>


<h3>Robustness</h3>
<p>We encode local information flowing the NCA using the fixed laplacian and gradient filters. As luck would have it, these can be defined for most underlying manifolds - giving us a way of placing our cells on various surfaces, in various configurations, without having to modify the learned model. Suppose we want our cells to live in a hexagonal world. We can redefine our kernels as follows:</p>

<p>Our model, trained in a purely square environment, works out of the box! Play the corresponding setting in the demo to experiment with this. </p>

<p> </p>
<h3>An Aside: Solitons and Lenia</h3>
<p>The motion of waves propagating through a medium can be described using the "classical" wave equation. The below equation defines the change of some quantity $u$ (be it the surface height map of a body of water, the position of a vibrating string, etc.) with respect to the laplacian of $u$. $c$ ends up being the propagation speed of the wave.</p>

<figure>
<p>$\ddot u &#x3D; c^2 \nabla^2 u$2</p>
</figure>

<p>One can imagine waves to come either as a sing]le wave, or as a larger mixture of waves of different frequencies - a phenomenon referred to as a "group" or a "packet". Physical phenomena, however, are rarely as structured and regular as we would like them to be. To describe real-world waves, such as waves in water, light waves propagating, or sound, we have somewhat more complex partial differential equations governing their motion. These waves all share a common property - that waves of different frequencies will travel at different speeds. "Speed" in such a context is a tricky thing to define - however in this case we are referring to the speed of any single given wave - how fast its peak travels in space. In the above, classic, wave equation this corresponds to $c$. However, in the real world, when waves of different frequencies have different speeds, groups of waves are no longer cohesive and will experience "dispersion" - the envelope of the group of waves will change shape over time and potentially not remain cohesive. Even in wave groups with dispersive properties, it is possible to find solutions to their partial differential equations where nonlinearities in the propagation and interaction between waves will counteract the dispersive properties of the wave group. This phenomenon, while lacking a strict definition, is called a "soliton" - as it describes a wave packet which exactly retains its shape during propagation.</p>

<p>Recall the idea briefly touched upon in Growing Neural Cellular Automata - that the communicating grid of CAs can be thought of as a finite difference approximation of a partial differential equation in both time and space, with the equation parametrized by a neural network. Several of the patterns we render in the pattern-generation experiment, as well as in  the inception experiment, consist of well defined structures such as circles or polygons. We consider such structures to be functionally equivalent to solitons and refer to them as such. Such a classification is inspired by solitons referred to in "Lenia" by B. Chan - solid, self-maintaining structures occurring as solutions to the continuous approximation to Game of Life.</p>
<h2 id='hidden-states'>Hidden states</h2>
<p>When biological cells communicate with each other, they do so through a multitude of available communication channels. Cells can emit or absorb different ions and proteins, sense physical motion or "stiffness" of other cells or even emit different chemical signals to diffuse over the local substrate. </p>

<p>There are various ways to visualize communication channels in real cells. One of them is to add to cells a potential-activated dye - allowing a clear picture of the voltage potential the cell is under with respect to the surrounding substrate. This technique has provided useful insight into the communication patterns within groups of cells - showing both local and nonlocal communication over a variety of time-scales.</p>

<p>As luck would have it - we can do something very similar with our Cellular Automata. Recall that our CA model consists of three visible channels, with the rest being treated as latent channels visible to the update step but excluded from any loss function. The video below visualizes the magnitudes of these hidden channels when mapped to intensity in RGB, for an inception NCA. Recall that hidden channels can be considered to be "floating" (to borrow a term from circuit theory) - they are not being pulled to any specific final state or intermediate state by the loss and instead converge to some form a dynamical system assisting the cell fulfill its "true" objective. This means there is no pre-defined assignment of different roles or meaning to different hidden channels, and there is almost certainly redundancy and correlation between different hidden channels. Such correlation may not be visible when applying an arbitrary grouping of channels in order to visualize them in RGB. However, even this simple analysis yields some interesting insights.</p>

<p>	</p>
<figure>
<video src='videos/threads_hidden.mov' autoplay loop muted style='width: 450px'></video>
<figcaption>An NCA trained to excite <b>mixed4b_70</b> in VGG.</figcaption>
</figure>

<p>The fourth quadrant above shows the RGB values of the NCA in the same way as we have been observing it before. The other three quadrants are visualizations of the hidden states. We can use this visualization technique to attempt some rudimentary qualitative analysis of what the NCA may be using the hidden channels for. For instance, in the third quadrant, we see a pattern very similarly to the visible channels. However, the &quot;threads&quot; pointing in each diagonal direction have different colours - one diagonal is an off-pink and the other is a yellow-ish colour. This suggests that one of the things encoded into the hidden states is the direction of a &quot;thread&quot;, likely to allow cells that are inside one of these threads to keep track of which direction the thread is growing, or moving, in. </p>

<p>Analysis of these hidden states is somewhat of a dark art and it is not always possible to draw rigorous conclusions about their role or behaviour. We welcome future work in this direction, as we believe qualitative analysis of these behaviours will be useful for understanding more complex behaviours of CAs.</p>
<h2 id='conclusion'>Conclusion</h2>
<p>Using NCAs as differentiable renderers, with certain style templates, or neuronal excitations as targets produces some very compelling results as well as very interesting and unexpected behaviour. Many of the solutions for generating certain patterns in an image appear similar to the underlying model or physical behaviour producing the pattern - the NCA seems to have inductive bias for treating &quot;objects&quot; in the pattern as individual objects (bubbles, for instance), letting them move freely across the space. The authors theorize that the NCA is forced to find algorithms that can produce such a pattern with purely local interaction. This constraint seems to result in an algorithm to ensure consistency and robustness at a higher level.</p>


</d-article>
<d-appendix>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
<d-appendix>