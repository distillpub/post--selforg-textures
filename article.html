<d-article>
{% include contents.html %}
{% include nextPrev.html %}
<p style="color:red;">This is a draft. Please do not share it further. If you want to comment or suggest changes, please ask access to this <a href="https://docs.google.com/document/d/1VX7tb6enGPlGm6VMYsX7uuDIb-L8YSCv3YYYLvdDSY4">Google Document</a>.</p>

<p>Neural Cellular Automata (NCA) are capable of learning a diverse set behaviours: from generating stable, regenerating, static images  <d-cite key="Mordvintsev_Randazzo_Niklasson_Levin_2020"></d-cite>, to segmenting images  <d-cite key="Sandler_Zhmoginov_Luo_Mordvintsev_Randazzo_Arcas_2020"></d-cite>, to learning to "self-classify" shapes <d-cite key="Randazzo_Mordvintsev_Niklasson_Levin_Greydanus_2020"></d-cite>. The inductive prior imposed by using cellular automata is powerful. A system of individual agents running the same learned local rule can solve surprisingly complex tasks, even when requiring coordination between cells across larger distances. By construction, they solve these tasks in a massively parallel and inherently degenerate<d-footnote> Degenerate in this case refers to the <a href='https://en.wikipedia.org/wiki/Degeneracy_(biology)'>biological concept of degeneracy</a>.</d-footnote> way - each cell must be able to take on the role of any other cell - as a result they tend to generalize well to unseen situations.</p>

<p>In this work, we task the NCA to learn to produce textures. A texture loss must by necessity allow for a degree of ambiguity. We investigate the subsequent learned behaviour, which has some surprising properties. We make the case that the cells learn distributed, local, algorithms. </p>

<p>To do this, we employ an old friend: differentiable parameterizations <d-cite key="Mordvintsev_Pezzotti_Schubert_Olah_2018"></d-cite>.</p>

<h2 id='patterns-textures-and-physical-processes'>Patterns, textures and physical processes</h2>
<p></p>
<figure>
<img src='images/zebra.jpg' style='width: 450px'>
<figcaption>A pair of Zebra. Zebra are said to have unique stripes.</figcaption>
</figure>

<p>Zebra stripes consist of a characteristic pattern. Ask almost anyone to identify zebra stripes in a set of images, and they&#39;ll have no trouble doing so. Ask them to describe what zebra stripes look like, and they will gladly tell you that they are parallel stripes of slightly varying width, alternating in black and white. At the same time, it is said that no two zebra have identical stripes<d-footnote> Perhaps an apocryphal claim, but at the very lowest level every zebra will be unique. Point is - "zebra stripes" as a concept in human understanding refers to the general structure of a black and white striped pattern, and not to a specific mapping from location to colour.</d-footnote>. One can then draw the conclusion that evolution has programmed the cells responsible for creating the zebra pattern to generate a pattern of a certain quality, with certain characteristics, as opposed to programming them with the blueprints for an exact bitmap of the edges and locations of stripes to be moulded to the surface of the Zebra's body.</p>

<p>Patterns and textures are ill-defined concepts. The Cambridge English Dictionary defines a pattern as &quot;any regularly repeated arrangement, especially a design made from repeated lines, shapes, or colours on a surface&quot;. This definition falls apart rather quickly when looking at patterns and textures that impart a feeling or quality, rather than a specific repeating property. A coloured fuzzy rug, for instance, can be considered a pattern or a texture, but is composed of strands pointing in random directions with small random variations in size and color, and thus no discernable regularity to the pattern. Penrose&#39; tilings do not repeat (they are not translationally invariant), but show them to anyone and they&#39;ll describe them as a pattern or a texture. Most patterns in nature are outputs of locally interacting processes that may or may not be stochastic in nature, but are often based upon fairly simple rules. There has been extensive work studying models which can give rise to such patterns in nature, with most work based on Turing&#39;s seminal paper on morphogenesis. <d-cite key="Turing_1990"></d-cite> </p>

<p>Such patterns are very common in developmental biology<d-cite key="Marcon_Sharpe_2012"></d-cite> . In addition to coat colors and skin pigmentation, invariant large-scale patterns (arising despite stochastic low-level detail) are a key feature of peripheral nerve networks, vascular networks, repeating units such as somites (blocks of tissue demarcated in embryogenesis that give rise to many organs), and segments of anatomical and genetic-level features, including whole body plans (e.g., snakes and centipedes) and appendages (such as demarcation of digit fields within the vertebrate limb<d-cite key="Schaerli_Munteanu_Gili_Cotterell_Sharpe_Isalan_2014"></d-cite><d-cite key="Hiscock_Tschopp_Tabin_2017"></d-cite><d-cite key="Raspopovic_Marcon_Russo_Sharpe_2014"></d-cite>). These kinds of patterns are generated by reaction-diffusion processes, bioelectric signaling, planar polarity, and other cell-cell communication mechanisms<d-cite key="Landge_Jordan_Diego_MÃ¼ller_2020"></d-cite><d-cite key="Pietak_Levin_2017"></d-cite><d-cite key="Brodsky"></d-cite><d-cite key="Goldbeter_2018"></d-cite>. Patterns in biology are not only structural, but also physiological, as in the waves of electrical activity in the brain, and the dynamics of gene regulatory networks which support computation sufficiently sophisticated as to be subject to Liar paradoxes . The emergence and control of such patterns are important not only to understand their evolutionary origin, but also to determine how these patterns are recognized (whether in the visual system of an animal looking at its environment, or in cells during regeneration ascertaining the degree of growth&#x2F;repair needed) and how they can be modulated in regenerative medicine settings.</p>

<p>As a result, when having any model learn to produce textures or patterns we want the model to learn a generation process for the pattern or a method to sample from the distribution governing this pattern. The first hurdle is to choose an appropriate loss function, or qualitative measure of the pattern. To do so, we employ ideas from Gatys et. al <d-cite key="Gatys_Ecker_Bethge_2015"></d-cite>. NCA become the parametrization for an image we &quot;stylize&quot; in the style of the target pattern. In this case, instead of having an existing image be re-styled in a new way, our starting point is completely unconstrained - it&#39;s the raw output of a randomly initialized NCA. The NCAs serve as the "renderer" or &quot;generator&quot;, and a pre-trained differentiable model serves as a distinguisher of the patterns - providing the gradient necessary for the renderer to learn to produce a pattern of a certain style.</p>
<h3 id='turing-to-cellular-automata-to-neural-networks'>Turing, to Cellular Automata, to Neural Networks</h3>
<p>NCA are well suited to generating textures. To understand why, we&#39;ll demonstrate parallels between texture generation in nature and NCA, evidencing that NCA are a reasonable model for learning texture generation.</p>

<h4 id='pdes'>PDEs</h4>
<p>In &quot;The Chemical Basis of Morphogenesis&quot; <d-cite key="Turing_1990"></d-cite>, Alan Turing proposed that simple physical processes of reaction and diffusion, modelled by partial differential equations, lie behind pattern formation in nature (such as the aforementioned Zebra stripes). Extensive work has since been done to identify PDEs modeling reaction-diffusion and evaluating their behaviour. One of the more celebrated examples is the Gray-Scott model of reaction diffusion (<d-cite key="Lee_McCormick_Ouyang_Swinney_1993"></d-cite>,<d-cite key="Pearson_1993"></d-cite>). This process has a veritable zoo of interesting behaviour, explorable by simply tuning the two parameters of the system. We strongly encourage readers to visit this excellent <a href='http://mrob.com/pub/comp/xmorphia/'>interactive atlas</a> of the different regions of the Gray-Scott reaction diffusion model to truly get a sense for the extreme variety of behaviour that&#39;s hidden behind two simple knobs. The more adventurous can even <a href='https://groups.csail.mit.edu/mac/projects/amorphous/jsim/sim/GrayScott.html'>play with a simulation locally</a> or <a href='https://mrob.com/pub/comp/xmorphia/ogl/index.html'>in the browser</a>.</p>

<p>A somewhat more general version of the above systems can be described by a simple Partial Differential Equation (PDE) over the state space of an image. </p>

<figure>
$\frac{\partial \mathbf{s} }{\partial  t } = f(\textbf{s}, \nabla_\mathbf{x} \textbf{s}, \nabla_\mathbf{x}^{2}\textbf{s})$
</figure>

<p>Here, $f$ is simply a function that depends on the gradient and laplacian of the state space and determines the time evolution of this state space. In less mathematical terms - we&#39;ve defined a system where every point of the image changes with time, in a way that depends on how the image currently changes in space. </p>
<h4 id='to-cas'>To CAs</h4>

<p>Differential equations governing natural phenomena are usually evaluated using numerical differential equation solvers. Indeed, this is sometimes the <strong>only</strong> way to solve them, as many PDEs and ODEs of interest don&#39;t have closed form solutions (often even deceptively simple ones, such as the <a href='https://en.wikipedia.org/wiki/Three-body_problem'>three-body problem</a>). Numerically solving PDEs and ODEs is a vast and well established field. The biggest and most important hammer in the metaphorical toolkit is establishing a regimen of discretizing the space of the ODE (or PDE). In the case of ODEs evaluating some phenomena in time, taking steps of fixed or varying size makes sense. </p>

<p>In our case, the logical approach is to discretize the continuous 2D image space into a 2D raster grid. Boundary conditions are, of course, to worry about, but we can solve this by moving to a toroidal world - where each dimension simply wraps around on itself.</p>

<p>Our next task is to discretize the gradient and laplacian operators - for this we use the <a href='https://en.wikipedia.org/wiki/Sobel_operator'>sobel operator</a> and the <a href='https://en.wikipedia.org/wiki/Discrete_Laplace_operator'>9-point variant</a> of the discrete laplace operator, as below. </p>

<figure>
$ \begin{array}{ c c c }
\begin{bmatrix}
-1 & 0 & 1\\-2 & 0 & 2 \\-1 & 0 & 1
\end{bmatrix}
&
\begin{bmatrix}
-1 & -2 & -1\\ 0 & 0 & 0 \\1 & 2 & 1
\end{bmatrix}
&
\begin{bmatrix}
1 & 2 & 1\\2 & -12 & 2 \\1 & 2 & 1
\end{bmatrix}
\\
Sobel_x & Sobel_y & Laplacian
\end{array}$
</figure>

<p>We now have a space-discretized version of our PDE that looks very much like a Cellular Automata - the time evolution of each discrete point in the raster grid depends only on its immediate neighbours. Indeed, we can now formalise our PDE in the form of an CA. </p>

<h4 id='to-neural-networks'>To Neural Networks</h4>
<p>The final step to solving or implementing the above general PDE for texture generation is to translate it to the &quot;language&quot; of deep learning. Fortunately, all the operations involved in iteratively evaluating the generalized PDE exist as common operations in most deep learning frameworks.  </p>

<h3 id='ncas-as-pattern-generators'>NCAs as pattern generators</h3>

<p></p>
<figure style='margin-left: auto; margin-right: auto; grid-column: page; width: 100%; max-width: 800px' >
<img src='images/texture_model.svg' style='width: 100%'>
<figcaption>Texture NCA model.</figcaption>
</figure>


<p>We build on the Growing CA NCA model <d-cite key="Mordvintsev_Randazzo_Niklasson_Levin_2020"></d-cite>, complete with built-in quantization of weights, and the batch pool mechanism to approximate long-term training. </p>
<h4 id='loss-function-'>Loss function: </h4>
<p>	</p>

<figure style='margin-left: auto; margin-right: auto; grid-column: page; width: 100%; max-width: 800px' >
<img src='images/texture_training.svg' style='width: 100%'>
<figcaption>Texture NCA model.</figcaption>
</figure>

<p>We use VGG as our differentiable discriminator of textures, for the same reasons outlined in Differentiable Parametrizations <d-cite key="Mordvintsev_Pezzotti_Schubert_Olah_2018"></d-cite>. We start with a template image, $\vec{x}$, we feed into VGG. We then collect statistics from certain layers (block{$1...6$}_conv1, block4_conv2) in the form of the raw activation values of the neurons in these layers. Finally, we feed the output of our Neural CA into VGG, after iterating the NCA for between 32 and 64 iterations. Our loss is the $L_2$ distance between the gram matrix of activations of these neurons with the NCA as input and their activations with the template image as input. We keep the weights of VGG frozen, and use ADAM <d-cite key="Kingma_Ba_2014"></d-cite> to update the weights of the NCA.</p>
<h4 id='dataset-'>Dataset: </h4>
<p>The template images for this dataset are from the Oxford Describable Textures Dataset <d-cite key="Cimpoi_Maji_Kokkinos_Mohamed_Vedaldi_2013"></d-cite>.  The aim of this dataset was to provide a training and testing set for algorithms to recognize textures and to describe textures using words. The textures were collected to match 47 &quot;attributes&quot; - such as &quot;bumpy&quot; or &quot;polka-dotted&quot;. These 47 attributes were in turn distilled from a set of common words used to describe textures identified by Bhusan, Rao and Lohse <d-cite key="Bhushan_Rao_Lohse_1997"></d-cite>. </p>
<h4 id='results'>Results:</h4>
<p>After a few iterations of training, we see the NCA converges to a solution similar in nature to the input template image, but not pixel-wise identical. The very first thing to notice is that the resulting solution found in the NCA parameters is <strong>not</strong> time-invariant - in other words - it&#39;s constantly changing! </p>

<p>This is not completely unexpected. In <i>Differentiable Parametrizations</i>, the authors noted that the images produced when backpropagating into the images space would end up different every time the algorithm was run, due to the stochastic nature of the parametrizations. To work around this, they introduce some tricks to keep <strong>alignment</strong> between several visualizations. In our model, we find that we attain such alignment in the temporal dimension without optimizing for it - a welcome surprise. We believe the reason is threefold. First, reaching and maintaining a static state is non-trivial, so we use the solution from Growing CA of maintaining a pool of NCA states at various iteration times and sampling these as starting states, to simulate loss being applied after a time period longer than the NCAs iteration period. Second, we apply our loss after a random number of iterations of the NCA - this means that at any given time step the pattern must be in a state that minimizes the loss. Thirdly, the stochastic updates, local communication and quantization all limit and regularize the magnitude of updates at each iteration, meaning the absolute difference between one iteration and the next is often not drastic. We hypothesize that this encourages the NCA to find a solution where each iteration is <strong>aligned</strong> with the previous iteration and thus the motion, or appearance of motion, we see in the trained NCA is the NCA traversing this manifold of locally aligned solutions. </p>

<p>We believe that finding temporally aligned solutions is equivalent to finding an algorithm, or process, that generates the template pattern. Below we demonstrate some exciting behaviours observed when training the NCA on different template images.  </p>

<p></p>
<figure>
<img src='images/chequered_0121.jpg' style='width: 325px'></img>
<video src='videos/grid.mp4' autoplay loop muted style='width: 325px'></video>
<figcaption>An NCA trained to create a pattern in the style of <b>chequered_0121.jpg</b>.</figcaption>
</figure>

<p>Here, we see that the NCA is trained using a template image of a simple black and white grid. </p>
<p>We notice that: </p>

<ul><li>Initially, a non-aligned grid of black and white quadrilaterals is formed. </li>
<li>As time progresses, the quadrilaterals seemingly grow or shrink in both $\vec{x}$ and $\vec{y}$ to more closely approximate squares. Quadrilaterals of both colours either emerge or disappear. Both of these behaviours seem to be an attempt to find local consistency. </li>
<li>After a longer time, the grid often ends up in perfect consistency.</li></ul>

<p>Such behaviour is not entirely unlike what one would expect in a hand-engineered algorithm to produce a consistent grid with local communication.</p>

<p></p>
<figure>
<img src='images/bubbly_0101.jpg' style='width: 325px'></img>
<video src='videos/bubbles.mp4' autoplay loop muted controls style='width: 325px'></video>
<figcaption>An NCA trained to create a pattern in the style of <b>bubbly_0101.jpg</b>.</figcaption>
</figure>

<p>In this video, the NCA has learned to reproduce a texture based off a template of clear bubbles on a blue background. One of the most interesting behaviours we observe is that the density of bubbles remains fairly constant. If we re-initialize the grid states, or interactively destroy states, we see a multitude of bubbles &quot;forming&quot;. However - as soon as two bubbles get too close to each other, one of them will spontaneously collapse and disappear, ensuring a constant density. We regard these bubbles as &quot;<a href='#an-aside-solitons-and-lenia'>solitons</a>&quot; in the solution space of our NCA - a concept we will discuss and investigate at length below.</p>

<p>If we speed the animation up, we see that different bubbles move at different speeds, yet they never collide or touch each other. Simultaneously, they are self-correcting - a damaged bubble will re-grow.</p>

<p>This behaviour is remarkable because it&#39;s behaviour that&#39;s spontaneously arisen without any external loss or auxiliary loss. All of these properties are learned from a combination of the template image, the information stored in the layers of VGG and the inductive bias of the NCA. The NCA has found a rule that effectively approximates many of the properties we expect of bubbles we see in this image - effectively it has learned a process that generates this pattern of bubbles in a robust and &quot;realistic&quot; way.</p>

<p></p>
<figure>
<img src='images/interlaced_0172.jpg' style='width: 325px'></img>
<video src='videos/viking.mp4'' autoplay loop muted controls style='width: 325px'></video>
<figcaption>An NCA trained to create a pattern in the style of <b>interlaced_0172.jpg</b>.</figcaption>
</figure>

<p>Here we see one of the authors&#39; favourite patterns - a simple geometric &quot;weave&quot;. Again, we notice the NCA seems to have learned an algorithm for producing this pattern. Each &quot;thread&quot; alternatively joins or detaches from other threads in order to produce the final pattern. This is strikingly similar to what one would attempt to implement in the case of being asked to programmatically generate the above pattern - some algorithm to stochastically weave threads together.</p>



<figure>
<img src='images/banded_0037.jpg' style='width: 325px'></img>
<video src='videos/lines.mp4' autoplay loop muted controls style='width: 325px'></video>
<figcaption>An NCA trained to create a pattern in the style of <b>banded_0037.jpg</b>.</figcaption>
</figure>

<p>This NCA also exhibits an algorithm for solving consistency in a striped pattern. The seams in ill-fitting stripes travel up, or down, the stripe, until either they merge to form a single straight stripe, or a stripe shrinks and disappears. Were this to be implemented algorithmically with local communication, it&#39;s not infeasible that a similar algorithm for finding consistency among the stripes would be used.</p>
<h3 id='related-work'>Related work</h3>
<p>Pattern generation is by no means a novel field. There has been extensive work predating deep-learning, in particular suggesting deep links between spatial patterning of anatomical structure and temporal patterning of cognitive and computational processes (e.g., reviewed in [[Pezzulo_Levin_2015]]@). Hans Spemann, one of the classical heroes of developmental biology, said "Again and again terms have been used which point not to physical but to psychical analogies. It was meant to be more than a poetical metaphor. It was meant to my conviction that the suitable reaction of a germ fragment, endowed with diverse potencies, in an embryonic &#39;field&#39;... is not a common chemical reaction, like all vital processes, are comparable, to nothing we know in such degree as to vital processes of which we have the most intimate knowledge." [[Speman_1938]].  More recently, Grossberg quantitatively laid out important similarities between developmental patterning and computational neuroscience[[Grossberg_1978]]. As briefly touched upon, the inspiration for much of the work came from Turing&#39;s work on pattern generation through local interaction, and later papers based on this principle. However, we also wish to acknowledge some works that we feel have kinship with ours. </p>
<h4 id='patch-sampling'>Patch sampling</h4>
<p>Early work in pattern generation focused on texture sampling. Patches were often sampled from the original image and reconstructed or rejoined in different ways to obtain an approximation of the texture. This method has also seen recent success with the work of Gumin <d-cite key="Gumin"></d-cite>.</p>
<h4 id='deep-learning'>Deep learning</h4>
<p>Gatys et. al&#39;s work <d-cite key="Gatys_Ecker_Bethge_2015"></d-cite>, referenced throughout, has been seminal with regards to the idea that statistics of certain layers in a pre-trained network can capture textures or styles in an image. There has been extensive work building on this idea, including playing with other parametrisations for the image generation <d-cite key="Mordvintsev_Pezzotti_Schubert_Olah_2018"></d-cite> and optimizing the generation process <d-cite key="Ulyanov_Lebedev_Vedaldi_Lempitsky_2016"></d-cite>. </p>

<p>Other work has focused on using a convolutional generator combined with path sampling and trained using an adversarial loss to produce a texture of similar quality <d-cite key="Xian_Sangkloy_Agrawal_Raj_Lu_Fang_Yu_Hays_2018"></d-cite>. </p>
<h4 id='interactive-evolution-of-camouflage'>Interactive Evolution of Camouflage</h4>
<p>Perhaps the most unconventional approach, with which we find nonetheless find kinship, is in Interactive Evolution of Camouflage <d-cite key="Reynolds_2011"></d-cite>, Craig Reynolds uses a texture description language, consisting of generators and operators, to parametrize a texture patch, which is presented to human viewers who have to decide which patches are the worst at &quot;camouflaging&quot; themselves against a chosen background texture. The population is updated in an evolutionary fashion to maximize &quot;camouflage&quot;, resulting in a texture exhibiting the most camouflage (to human eyes) after a number of iterations. We see strong parallels to our work - instead of a texture generation language, we have an NCA parametrize the texture, and instead of human reviewers we use VGG as an evaluator of the quality of a generated pattern. We believe a fundamental difference lies in the solution space of an NCA. A texture generation language comes with a number of inductive biases and learns a deterministic mapping from coordinate to colour. Our method appears to learn more general algorithms and behaviours giving rise to the target pattern.</p>

<p>Two other noteworthy examples of similar work are Portilla et. al&#39;s work with the wavelet transform <d-cite key="Portilla_Simoncelli_2000"></d-cite>, and work by Chen et al with reaction diffusion <d-cite key="Chen_Pock_2017"></d-cite>.</p>
<h2 id='feature-visualization'>Feature Visualization</h2>

<p></p>
<figure>
<img src='images/butterfly_eye.jpg' style='width: 450px'></img>
<figcaption>A butterfly with an "eye-spot" on the wings.</figcaption>
</figure>

<p>We&#39;ve now explored some of the fascinating behaviours learned by the NCA when presented with a template image. What if we want to see them learn even more &quot;unconstrained&quot; behaviour? </p>

<p>Some butterflies have remarkably lifelike eyes on their wings. It&#39;s unlikely the butterflies are even aware of this incredible artwork on their own bodies. Evolution placed these there to trigger a response of fear in potential predators. It is likely that neither the predator nor the butterfly has a concept for what an eye is or what an eye does, or even less so any <a href='https://en.wikipedia.org/wiki/Theory_of_mind'>theory of mind</a> regarding the consciousness of the other, but evolution has identified a region of morphospace for this organism lineage that exploits pattern-identifying features of predators to</p>
<p> trick them into fearing a harmless bug instead of consuming it. </p>

<p>Even more remarkable is the fact that the individual cells composing the butterfly's wings can self assemble into coherent, beautiful, shapes far larger than an individual cell - indeed a cell is on the order of $1^{-5}m$ <d-cite key="Ohno_Otaki_2015"></d-cite> while the features on the wings will grow to as large as $1^{-3}m$ <d-cite key="Iwata_Otaki_2016"></d-cite>. Communication over this distance implies a self-organization over a distance of hundreds or thousands of cells to generate a coherent image of an eye that evolved simply to act as a visual stimuli for an entirely different species. Of course, this pales in comparison to the morphogenesis that occurs in animal and plant bodies, where structures consisting of millions of cells will specialize and coordinate to generate the target morphology. </p>

<p>A common approach to investigating neural networks is to look at what inhibits or excites individual neurons in a network <d-cite key="Schubert_Petrov_Carter_Cammarata_Goh_Olah_2020"></d-cite>. Just as neuroscientists and biologists have often treated cells and cell structures and neurons as black-box models to be investigated, measured and reverse-engineered, there is a large contemporary body of work on doing the same with neural networks. For instance the work by Boettiger.  [[Boettiger_Ermentrout_Oster_2009]] <d-cite key="Boettiger_Oster_2009"></d-cite>.</p>

<p>We can explore this idea with minimal effort by taking our VGG implementation from when we explored pattern generation, and exploring what happens if we task the NCA to enter a state that excites a given neuron in VGG. One of the common resulting NCAs we notice is eye and eye-related shapes - such as the video below - likely as a result of having to detect various animals in ImageNet. Much like the cells forming the eyes on the wings of butterflies to excite certain neurons in the brains of predators, our NCA has learned to have cells collaborate to produce a pattern exciting certain neurons in an arbitrary external neural network.</p>

<p></p>
<figure>
<img src='images/mixed4a_472_microscope.png' style='width: 325px'></img>
<video src='videos/eyes.mp4' autoplay loop muted controls style='width: 325px'></video>
<figcaption>An NCA trained to excite <b>mixed4a_472</b> in VGG.</figcaption>
</figure>

<h3 id='experiment-feature-visualization'>Experiment: Feature Visualization</h3>
<h4 id='model-'>Model: </h4>
<p>We use a model identical to the one used for exploring pattern generation. </p>
<h4 id='loss-function-'>Loss function: </h4>
<p>We use VGG, as before. Our loss maximizes the activations of the chosen neurons in VGG, when evaluated on the output of the NCA. We add an auxiliary loss to encourage the outputs of the NCA to be $\in [0,1]$, as this is not inherently built into the model. We keep the weights of VGG frozen, and use ADAM <d-cite key="Kingma_Ba_2014"></d-cite> to update the weights of the NCA.</p>
<h4 id='dataset-'>Dataset: </h4>
<p>There is no explicit dataset for this task. VGG is trained on ImageNet. The layers and neurons we chose to excite are sampled qualitatively from OpenAI Microscope.</p>
<h4 id='results'>Results:</h4>
<p>Similar to the pattern generation experiment, we see quick convergence and a tendency to find temporally dynamic solutions - i.e. most resulting NCAs don&#39;t stay still.</p>

<h2 id='discussion'>Discussion</h2>
<h3 id='interesting-behaviours-of-inception-solitons'>Interesting behaviours of inception solitons</h3>
<p></p>
<figure>
<img src='images/mixed4c_439_microscope.png' style='width: 325px'></img>
<video src='videos/eyes.mov' autoplay loop muted controls style='width: 325px'></video>
<figcaption>An NCA trained to excite <b>mixed4c_439</b> in VGG.</figcaption>
</figure>
<p>Solitons in the form of regular circle-like shapes with internal structure are quite commonly observed in the inception renderings. Two solitons approaching each other too closely may cause one of both of them to decay. We also observe that solitons can "divide" into two complete new solitons.</p>
<p></p>
<figure>
<img src='images/mixed3b_454_microscope.png' style='width: 325px'></img>
<video src='videos/moving_thread.mov' autoplay loop muted controls style='width: 325px'></video>
<figcaption>An NCA trained to excite <b>mixed3b_454</b> in VGG.</figcaption>
</figure>
<p>In textures that are composed of threads or lines, or in certain excitations of VGG neurons where the resulting NCA has a &quot;thread-like&quot; quality, the threads grow in their respective directions and will join other threads, or grow around them, as required. This behaviour is similar to the regular lines observed in the striped patterns during pattern generation.</p>


<h3 id='robustness'>Robustness</h3>
<h4 id='switching-manifold'>Switching manifold</h4>

<p>We encode local information flowing the NCA using the fixed laplacian and gradient filters. As luck would have it, these can be defined for most underlying manifolds - giving us a way of placing our cells on various surfaces, in various configurations, without having to modify the learned model. Suppose we want our cells to live in a hexagonal world. We can redefine our kernels as follows:</p>

<p>Our model, trained in a purely square environment, works out of the box on a hexagonal grid! Play the corresponding setting in the demo to experiment with this. </p>

<p> </p>
<figure>
<img src='images/coral_square.png' style='width: 450px'></img>
<img src='images/coral_hex.png' style='width: 450px'></img>
<figcaption>The same texture evaluated on a square and hexagonal grid, respectively.</figcaption>
</figure>
<h4 id='timesynchronization'>Time-synchronization</h4>
<p></p>
<figure>
<video src='videos/time_unsynced.mp4' autoplay loop controls muted style='width: 450px'></video>
<figcaption>Two NCAs running next to each other, at different speeds. They can communicate through their shared edge.</figcaption>
</figure>

<p>Stochastic updates teach the cells to be somewhat robust to asynchronous updates. We investigate this property by taking it to an extreme - how do the cells react if two manifolds are allowed to communicate, but one runs the NCA at a different speed than the other? The result is surprisingly stable - the CA is still able to construct and maintain a consistent texture across the combined manifold. </p>
<h3 id='hidden-states'>Hidden states</h3>
<p>When biological cells communicate with each other, they do so through a multitude of available communication channels. Cells can emit or absorb different ions and proteins, sense physical motion or "stiffness" of other cells or even emit different chemical signals to diffuse over the local substrate. </p>

<p>There are various ways to visualize communication channels in real cells. One of them is to add to cells a potential-activated dye - allowing a clear picture of the voltage potential the cell is under with respect to the surrounding substrate. This technique has provided useful insight into the communication patterns within groups of cells - showing both local and nonlocal communication over a variety of time-scales.</p>

<p>As luck would have it - we can do something very similar with our Cellular Automata. Recall that our CA model consists of three visible channels, with the rest being treated as latent channels visible to the update step but excluded from any loss function. Below we visualize the values of hidden channels in the first three principle components, when mapped to intensity in RGB, for an inception NCA. Recall that hidden channels can be considered to be "<a href='https://en.wikipedia.org/wiki/Floating_ground'>floating</a>" (to abuse a term from circuit theory) - they are not being pulled to any specific final state or intermediate state by the loss and instead converge to some form a dynamical system assisting the cell fulfill its "true" objective. This means there is no pre-defined assignment of different roles or meaning to different hidden channels, and there is almost certainly redundancy and correlation between different hidden channels. Such correlation may not be visible when applying an arbitrary grouping of channels in order to visualize them in RGB. However, even this simple analysis yields some interesting insights.</p>

<p>	</p>
<figure>
<video src='videos/hidden_pca.mp4' autoplay loop muted controls style='width: 650px'></video>
<figcaption><b>Left:</b> RGB channels of NCA. <b>Right:</b> Intensities of top three principal components of hidden states. <br><br> An NCA trained to excite <b>mixed4b_70</b> in VGG. Notice the hidden states appear to encode information about structure. "Threads" along the major diagonal (NW - SE) appear primarily green, while those running along the anti-diagonal appear blue, indicating that these have differing internal states, despite being effectively indistinguishable in RGB space.</figcaption>
</figure>

<p>The right frame shows a visualisation of PCA-transformed hidden states of the cells. Specifically - we find the top three components and visualize  The other three quadrants are visualizations of the hidden states. We can use this visualization technique to attempt some rudimentary qualitative analysis of what the NCA may be using the hidden channels for. For instance, in the bottom-left quadrant, we see a pattern very similarly to the visible channels. However, the &quot;threads&quot; pointing in each diagonal direction have different colours - one diagonal is an off-pink and the other is a yellow-ish colour. This suggests that one of the things encoded into the hidden states is the direction of a &quot;thread&quot;, likely to allow cells that are inside one of these threads to keep track of which direction the thread is growing, or moving, in. </p>
<p></p>
<figure>
<video src='videos/chequered_hidden.mp4' autoplay loop muted controls style='width: 650px'></video>
<figcaption><b>Left:</b> RGB channels of NCA. <b>Right:</b> Intensities of top three principal components of hidden states. <br><br> An NCA trained to produce a texture based on DTD image <b>cheqeuered_0121</b>. Notice the structure of squares - with a gradient occurring inside the structure of each square, evidencing that structure is being encoded in hidden state.</figcaption>
</figure>
<p></p>
<figure>
<video src='videos/eyes_hidden.mp4' autoplay loop muted controls style='width: 650px'></video>
<figcaption><b>Left:</b> RGB channels of NCA. <b>Right:</b> Intensities of top three principal components of hidden states. <br><br>  An NCA trained to excite <b>mixed4c_208</b> in VGG. The visible body of the eye is clearly demarcated in the hidden states. There is also a "halo" which appears to modulate growth of any solitons immediately next to each other. This halo is barely visible in the RGB channels.</figcaption>
</figure> 

<p>Analysis of these hidden states is somewhat of a dark art and it is not always possible to draw rigorous conclusions about their role or behaviour. We welcome future work in this direction, as we believe qualitative analysis of these behaviours will be useful for understanding more complex behaviours of CAs.</p>
<h2 id='conclusion'>Conclusion</h2>
<p>Using NCAs as differentiable renderers, with certain style templates, or neuronal excitations as targets produces some very compelling results as well as very interesting and unexpected behaviour. Many of the solutions for generating certain patterns in an image appear similar to the underlying model or physical behaviour producing the pattern - the NCA seems to have inductive bias for treating &quot;objects&quot; in the pattern as individual objects (bubbles, for instance), letting them move freely across the space. The NCA is forced to find algorithms that can produce such a pattern with purely local interaction. This constraint seems to result in an algorithm to ensure consistency and robustness at a higher level.</p>


</d-article>
<d-appendix>
<h3 id='an-aside-solitons-and-lenia'>An Aside: Solitons and Lenia</h3>
<p>The motion of waves propagating through a medium can be described using the "classical" wave equation. The below equation defines the change of some quantity $u$ (be it the surface height map of a body of water, the position of a vibrating string, etc.) with respect to the laplacian of $u$. $c$ ends up being the propagation speed of the wave.</p>

<figure>
<p>$\ddot u &#x3D; c^2 \nabla^2 u$2</p>
</figure>

<p>One can imagine waves to come either as a single wave, or as a larger mixture of waves of different frequencies - a phenomenon referred to as a "group" or a "packet". Physical phenomena, however, are rarely as structured and regular as we would like them to be. To describe real-world waves, such as waves in water, light waves propagating, or sound, we have somewhat more complex partial differential equations governing their motion. These waves all share a common property - that waves of different frequencies will travel at different speeds. "Speed" in such a context is a tricky thing to define - however in this case we are referring to the speed of any single given wave - how fast its peak travels in space. In the above, classic, wave equation this corresponds to $c$. However, in the real world, when waves of different frequencies have different speeds, groups of waves are no longer cohesive and will experience "dispersion" - the envelope of the group of waves will change shape over time and potentially not remain cohesive. Even in wave groups with dispersive properties, it is possible to find solutions to their partial differential equations where nonlinearities in the propagation and interaction between waves will counteract the dispersive properties of the wave group. This phenomenon, while lacking a strict definition, is called a "soliton" - as it describes a wave packet which exactly retains its shape during propagation.</p>

<p>Recall the idea briefly touched upon in Growing Neural Cellular Automata - that the communicating grid of CAs can be thought of as a finite difference approximation of a partial differential equation in both time and space, with the equation parametrized by a neural network. Several of the patterns we render in the pattern-generation experiment, as well as in  the inception experiment, consist of well defined structures such as circles or polygons. We consider such structures to be functionally equivalent to solitons and refer to them as such. Such a classification is inspired by solitons referred to in "Lenia" by B. Chan - solid, self-maintaining structures occurring as solutions to the continuous approximation to Game of Life.</p>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
<d-appendix>