 @article{Bhushan_Rao_Lohse_1997, title={The texture lexicon: Understanding the categorization of visual texture terms and their relationship to texture images}, volume={21}, ISSN={0364-0213}, url={http://doi.wiley.com/10.1207/s15516709cog2102_4}, DOI={10.1207/s15516709cog2102_4}, abstractNote={In this paper we present the results of two experiments. The first is on the categorization of texture words in the English language. The goal was to determine whether there is a common basis for subjects’ groupings of words related to visual texture, and if so, to identify the underlying dimensions used to categorize those words. Eleven major clusters were identified through hierarchical cluster analysis, ranging from ?random? to ?repetitive?. These clusters remained intact in a multidimensional scaling solution. The stress for a three-dimensional solution obtained through multidimensional scaling was 0.18, meaning that 82% of the variance in the data is explained through the use of three dimensions. It appears that the major dimensions of texture descriptors are repetitive versus nonrepetitive; linearly oriented versus circularly oriented; and simple versus complex. In the second experiment we measured the strength of association between texture words and texture images. The goal was to determine whether there is any systematic correspondence between the domains of texture words and texture images. Pearson's coefficient of contingency, a measure of the strength of association, was found to be 0.63 for words corresponding to given images and 0.56 for images corresponding to given words. Thus the texture categories in the verbal space and those in the visual space are strongly tied. In sum, our two experiments show (a) that despite the tremendous variety in the words we have to describe textures, there is an underlying structure to the lexical space which can be derived from the experimental data; and (b) that the association between a category of words and a category of images was strongest when both categories represent the same underlying property. This suggests that subjects' organizations of texture terms are systematically tied to their organization of texture images.}, number={2}, journal={Cognitive science}, publisher={Wiley}, author={Bhushan, Nalini and Rao, A. Ravishankar and Lohse, Gerald L.}, year={1997}, month={Apr}, pages={219–246} }

 @article{Chen_Pock_2017, title={Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration}, volume={39}, ISSN={0162-8828}, url={http://dx.doi.org/10.1109/TPAMI.2016.2596743}, DOI={10.1109/TPAMI.2016.2596743}, abstractNote={Image restoration is a long-standing problem in low-level computer vision with many interesting applications. We describe a flexible learning framework based on the concept of nonlinear reaction diffusion models for various image restoration problems. By embodying recent improvements in nonlinear diffusion models, we propose a dynamic nonlinear reaction diffusion model with time-dependent parameters (i.e., linear filters and influence functions). In contrast to previous nonlinear diffusion models, all the parameters, including the filters and the influence functions, are simultaneously learned from training data through a loss based approach. We call this approach TNRD-Trainable Nonlinear Reaction Diffusion. The TNRD approach is applicable for a variety of image restoration tasks by incorporating appropriate reaction force. We demonstrate its capabilities with three representative applications, Gaussian image denoising, single image super resolution and JPEG deblocking. Experiments show that our trained nonlinear diffusion models largely benefit from the training of the parameters and finally lead to the best reported performance on common test datasets for the tested applications. Our trained models preserve the structural simplicity of diffusion models and take only a small number of diffusion steps, thus are highly efficient. Moreover, they are also well-suited for parallel computation on GPUs, which makes the inference procedure extremely fast.}, number={6}, journal={IEEE transactions on pattern analysis and machine intelligence}, author={Chen, Yunjin and Pock, Thomas}, year={2017}, month={Jun}, pages={1256–1272} }

 @unpublished{Cimpoi_Maji_Kokkinos_Mohamed_Vedaldi_2013, title={Describing Textures in the Wild}, url={http://arxiv.org/abs/1311.3618}, abstractNote={Patterns and textures are defining characteristics of many natural objects: a shirt can be striped, the wings of a butterfly can be veined, and the skin of an animal can be scaly. Aiming at supporting this analytical dimension in image understanding, we address the challenging problem of describing textures with semantic attributes. We identify a rich vocabulary of forty-seven texture terms and use them to describe a large dataset of patterns collected in the wild.The resulting Describable Textures Dataset (DTD) is the basis to seek for the best texture representation for recognizing describable texture attributes in images. We port from object recognition to texture recognition the Improved Fisher Vector (IFV) and show that, surprisingly, it outperforms specialized texture descriptors not only on our problem, but also in established material recognition datasets. We also show that the describable attributes are excellent texture descriptors, transferring between datasets and tasks; in particular, combined with IFV, they significantly outperform the state-of-the-art by more than 8 percent on both FMD and KTHTIPS-2b benchmarks. We also demonstrate that they produce intuitive descriptions of materials and Internet images.}, journal={arXiv [cs.CV]}, author={Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea}, year={2013}, month={Nov} }

 @unpublished{Gatys_Ecker_Bethge_2015, title={Texture Synthesis Using Convolutional Neural Networks}, url={http://arxiv.org/abs/1505.07376}, abstractNote={Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.}, journal={arXiv [cs.CV]}, author={Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias}, year={2015}, month={May} }

 @book{Gumin, title={WaveFunctionCollapse}, url={https://github.com/mxgmn/WaveFunctionCollapse}, abstractNote={Bitmap & tilemap generation from a single example with the help of ideas from quantum mechanics - mxgmn/WaveFunctionCollapse}, publisher={Github}, author={Gumin, Maxim} }

 @article{Iwata_Otaki_2016, title={Focusing on butterfly eyespot focus: uncoupling of white spots from eyespot bodies in nymphalid butterflies}, volume={5}, ISSN={2193-1801}, url={http://dx.doi.org/10.1186/s40064-016-2969-8}, DOI={10.1186/s40064-016-2969-8}, abstractNote={BACKGROUND: Developmental studies on butterfly wing color patterns often focus on eyespots. A typical eyespot (such as that of Bicyclus anynana) has a few concentric rings of dark and light colors and a white spot (called a focus) at the center. The prospective eyespot center during the early pupal stage is known to act as an organizing center. It has often been assumed, according to gradient models for positional information, that a white spot in adult wings corresponds to an organizing center and that the size of the white spot indicates how active that organizing center was. However, there is no supporting evidence for these assumptions. To evaluate the feasibility of these assumptions in nymphalid butterflies, we studied the unique color patterns of Calisto tasajera (Nymphalidae, Satyrinae), which have not been analyzed before in the literature. RESULTS: In the anterior forewing, one white spot was located at the center of an eyespot, but another white spot associated with either no or only a small eyespot was present in the adjacent compartment. The anterior hindwing contained two adjacent white spots not associated with eyespots, one of which showed a sparse pattern. The posterior hindwing contained two adjacent pear-shaped eyespots, and the white spots were located at the proximal side or even outside the eyespot bodies. The successive white spots within a single compartment along the midline in the posterior hindwing showed a possible trajectory of a positional determination process for the white spots. Several cases of focus-less eyespots in other nymphalid butterflies were also presented. CONCLUSIONS: These results argue for the uncoupling of white spots from eyespot bodies, suggesting that an eyespot organizing center does not necessarily differentiate into a white spot and that a prospective white spot does not necessarily signify organizing activity for an eyespot. Incorporation of these results in future models for butterfly wing color pattern formation is encouraged.}, number={1}, journal={SpringerPlus}, author={Iwata, Masaki and Otaki, Joji M.}, year={2016}, month={Aug}, pages={1287} }

 @unpublished{Kingma_Ba_2014, title={Adam: A Method for Stochastic Optimization}, url={http://arxiv.org/abs/1412.6980}, abstractNote={We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}, journal={arXiv [cs.LG]}, author={Kingma, Diederik P. and Ba, Jimmy}, year={2014}, month={Dec} }

 @article{Lee_McCormick_Ouyang_Swinney_1993, title={Pattern formation by interacting chemical fronts}, volume={261}, ISSN={0036-8075}, url={http://dx.doi.org/10.1126/science.261.5118.192}, DOI={10.1126/science.261.5118.192}, abstractNote={Experiments on a bistable chemical reaction in a continuously fed thin gel layer reveal a new type of spatiotemporal pattern, one in which fronts propagate at a constant speed until they reach a critical separation (typically 0.4 millimeter) and stop. The resulting asymptotic state is a highly irregular stationary pattern that contrasts with the regular patterns such as hexagons, squares, and stripes that have been observed in many nonequilibrium systems. The observed patterns are initiated by a finite amplitude perturbation rather than through spontaneous symmetry breaking.}, number={5118}, journal={Science}, author={Lee, K. J. and McCormick, W. D. and Ouyang, Q. and Swinney, H. L.}, year={1993}, month={Jul}, pages={192–194} }

 @article{Mordvintsev_Pezzotti_Schubert_Olah_2018, title={Differentiable Image Parameterizations}, volume={3}, ISSN={2476-0757}, url={https://distill.pub/2018/differentiable-parameterizations}, DOI={10.23915/distill.00012}, number={7}, journal={Distill}, author={Mordvintsev, Alexander and Pezzotti, Nicola and Schubert, Ludwig and Olah, Chris}, year={2018}, month={Jul} }

 @article{Mordvintsev_Randazzo_Niklasson_Levin_2020, title={Growing Neural Cellular Automata}, volume={5}, ISSN={2476-0757}, url={https://distill.pub/2020/growing-ca}, DOI={10.23915/distill.00023}, number={2}, journal={Distill}, author={Mordvintsev, Alexander and Randazzo, Ettore and Niklasson, Eyvind and Levin, Michael}, year={2020}, month={Feb}, pages={e23} }

 @article{Ohno_Otaki_2015, title={Live Cell Imaging of Butterfly Pupal and Larval Wings In Vivo}, volume={10}, ISSN={1932-6203}, url={http://dx.doi.org/10.1371/journal.pone.0128332}, DOI={10.1371/journal.pone.0128332}, abstractNote={Butterfly wing color patterns are determined during the late larval and early pupal stages. Characterization of wing epithelial cells at these stages is thus critical to understand how wing structures, including color patterns, are determined. Previously, we successfully recorded real-time in vivo images of developing butterfly wings over time at the tissue level. In this study, we employed similar in vivo fluorescent imaging techniques to visualize developing wing epithelial cells in the late larval and early pupal stages 1 hour post-pupation. Both larval and pupal epithelial cells were rich in mitochondria and intracellular networks of endoplasmic reticulum, suggesting high metabolic activities, likely in preparation for cellular division, polyploidization, and differentiation. Larval epithelial cells in the wing imaginal disk were relatively large horizontally and tightly packed, whereas pupal epithelial cells were smaller and relatively loosely packed. Furthermore, larval cells were flat, whereas pupal cells were vertically elongated as deep as 130 μm. In pupal cells, many endosome-like or autophagosome-like structures were present in the cellular periphery down to approximately 10 μm in depth, and extensive epidermal feet or filopodia-like processes were observed a few micrometers deep from the cellular surface. Cells were clustered or bundled from approximately 50 μm in depth to deeper levels. From 60 μm to 80 μm in depth, horizontal connections between these clusters were observed. The prospective eyespot and marginal focus areas were resistant to fluorescent dyes, likely because of their non-flat cone-like structures with a relatively thick cuticle. These in vivo images provide important information with which to understand processes of epithelial cell differentiation and color pattern determination in butterfly wings.}, number={6}, journal={PloS one}, author={Ohno, Yoshikazu and Otaki, Joji M.}, year={2015}, month={Jun}, pages={e0128332} }

 @article{Pearson_1993, title={Complex patterns in a simple system}, volume={261}, ISSN={0036-8075}, url={http://dx.doi.org/10.1126/science.261.5118.189}, DOI={10.1126/science.261.5118.189}, abstractNote={Numerical simulations of a simple reaction-diffusion model reveal a surprising variety of irregular spatiotemporal patterns. These patterns arise in response to finite-amplitude perturbations. Some of them resemble the steady irregular patterns recently observed in thin gel reactor experiments. Others consist of spots that grow until they reach a critical size, at which time they divide in two. If in some region the spots become overcrowded, all of the spots in that region decay into the uniform background.}, number={5118}, journal={Science}, author={Pearson, J. E.}, year={1993}, month={Jul}, pages={189–192} }

 @misc{Portilla_Simoncelli_2000, title={A parametric texture model based on joint statistics of complex wavelet coefficients}, url={https://www.cns.nyu.edu/pub/eero/portilla99-reprint.pdf}, author={Portilla, Javier and Simoncelli, Eero P.}, year={2000} }

 @article{Randazzo_Mordvintsev_Niklasson_Levin_Greydanus_2020, title={Self-classifying MNIST Digits}, volume={5}, ISSN={2476-0757}, url={https://distill.pub/2020/selforg/mnist}, DOI={10.23915/distill.00027.002}, number={8}, journal={Distill}, author={Randazzo, Ettore and Mordvintsev, Alexander and Niklasson, Eyvind and Levin, Michael and Greydanus, Sam}, year={2020}, month={Aug} }

 @article{Reynolds_2011, title={Interactive evolution of camouflage}, volume={17}, ISSN={1064-5462}, url={http://dx.doi.org/10.1162/artl_a_00023}, DOI={10.1162/artl_a_00023}, abstractNote={This article presents an abstract computation model of the evolution of camouflage in nature. The 2D model uses evolved textures for prey, a background texture representing the environment, and a visual predator. A human observer, acting as the predator, is shown a cohort of 10 evolved textures overlaid on the background texture. The observer clicks on the five most conspicuous prey to remove (“eat”) them. These lower-fitness textures are removed from the population and replaced with newly bred textures. Biological morphogenesis is represented in this model by procedural texture synthesis. Nested expressions of generators and operators form a texture description language. Natural evolution is represented by genetic programming (GP), a variant of the genetic algorithm. GP searches the space of texture description programs for those that appear least conspicuous to the predator.}, number={2}, journal={Artificial life}, author={Reynolds, Craig}, year={2011}, month={Mar}, pages={123–136} }

 @unpublished{Sandler_Zhmoginov_Luo_Mordvintsev_Randazzo_Arcas_2020, title={Image segmentation via Cellular Automata}, url={http://arxiv.org/abs/2008.04965}, abstractNote={In this paper, we propose a new approach for building cellular automata to solve real-world segmentation problems. We design and train a cellular automaton that can successfully segment high-resolution images. We consider a colony that densely inhabits the pixel grid, and all cells are governed by a randomized update that uses the current state, the color, and the state of the $3times 3$ neighborhood. The space of possible rules is defined by a small neural network. The update rule is applied repeatedly in parallel to a large random subset of cells and after convergence is used to produce segmentation masks that are then back-propagated to learn the optimal update rules using standard gradient descent methods. We demonstrate that such models can be learned efficiently with only limited trajectory length and that they show remarkable ability to organize the information to produce a globally consistent segmentation result, using only local information exchange. From a practical perspective, our approach allows us to build very efficient models -- our smallest automaton uses less than 10,000 parameters to solve complex segmentation tasks.}, journal={arXiv [cs.CV]}, author={Sandler, Mark and Zhmoginov, Andrey and Luo, Liangcheng and Mordvintsev, Alexander and Randazzo, Ettore and Arcas, Blaise Agúera y.}, year={2020}, month={Aug} }

 @misc{Schubert_Petrov_Carter_Cammarata_Goh_Olah_2020, title={OpenAI Microscope}, url={https://openai.com/blog/microscope/}, abstractNote={We’re introducing OpenAI Microscope, a collection of visualizations of every significant layer and neuron of eight vision “model organisms” which are often studied in interpretability.}, publisher={OpenAI}, author={Schubert, Ludwig and Petrov, Michael and Carter, Shan and Cammarata, Nick and Goh, Gabriel and Olah, Chris}, year={2020}, month={Apr} }

 @article{Turing_1990, title={The chemical basis of morphogenesis. 1953}, volume={52}, ISSN={0092-8240}, url={http://dx.doi.org/10.1007/BF02459572}, DOI={10.1007/BF02459572}, number={1-2}, journal={Bulletin of mathematical biology}, author={Turing, A. M.}, year={1990}, pages={153–97; discussion 119–52} }

 @unpublished{Ulyanov_Lebedev_Vedaldi_Lempitsky_2016, title={Texture Networks: Feed-forward Synthesis of Textures and Stylized Images}, url={http://arxiv.org/abs/1603.03417}, abstractNote={Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods requires a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys~et~al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.}, journal={arXiv [cs.CV]}, author={Ulyanov, Dmitry and Lebedev, Vadim and Vedaldi, Andrea and Lempitsky, Victor}, year={2016}, month={Mar} }

 @inproceedings{Xian_Sangkloy_Agrawal_Raj_Lu_Fang_Yu_Hays_2018, title={TextureGAN: Controlling deep image synthesis with texture patches}, ISBN={9781538664209}, url={https://openaccess.thecvf.com/content_cvpr_2018/papers/Xian_TextureGAN_Controlling_Deep_CVPR_2018_paper.pdf}, DOI={10.1109/cvpr.2018.00882}, booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, publisher={IEEE}, author={Xian, Wenqi and Sangkloy, Patsorn and Agrawal, Varun and Raj, Amit and Lu, Jingwan and Fang, Chen and Yu, Fisher and Hays, James}, year={2018}, month={Jun} }
