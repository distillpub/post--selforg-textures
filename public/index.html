<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style>
  </style>
  <script src="twgl.min.js"></script>
  <script type="module" src="./ca.js"></script>
  <script type="module" src="./demo.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">{
      "title": "Self-Organising Textures",
      "description": "Training an end-to-end differentiable, self-organising cellular automata for texture synthesius.",
      "authors": [
        {
          "author": "Alexander Mordvintsev",
          "authorURL": "https://znah.net/",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Ettore Randazzo",
          "authorURL": "",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Eyvind Niklasson",
          "authorURL": "https://eyvind.me/",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Michael Levin",
          "authorURL": "http://www.drmichaellevin.org",
          "affiliation": "Allen Discovery Center at Tufts University",
          "affiliationURL": "http://allencenter.tufts.edu"
        }
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <style>
    figure {
      text-align: center;
      margin-bottom: 0.5em;
      margin-top: 0.5em;
    }
    figure img {
      max-width: 100%;
      width: unset;
    }
    video {
      max-width: 100%;
    }
    .colab-root {
      display: inline-block;
      background: rgba(255, 255, 255, 0.75);
      padding: 4px 8px;
      border-radius: 4px;
      font-size: 11px!important;
      text-decoration: none;
      color: #aaa;
      border: none;
      font-weight: 300;
      border: solid 1px rgba(0, 0, 0, 0.08);
      border-bottom-color: rgba(0, 0, 0, 0.15);
      text-transform: uppercase;
      line-height: 16px;
    }

   span.colab-span {
      background-image: url(images/colab.svg);
      background-repeat: no-repeat;
      background-size: 20px;
      background-position-y: 2px;
      display: inline-block;
      padding-left: 24px;
      border-radius: 4px;
      text-decoration: none;
    }

    a.colab-root:hover{
      color: #666;
      background: white;
      border-color: rgba(0, 0, 0, 0.2);
    }

    /* TOC */
    @media(max-width: 1000px){
      d-contents {
        justify-self: start;
        align-self: start;
        grid-column-start: 2;
        grid-column-end: 6;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
      }
    } 
    
    @media (min-width: 1000px){
      d-contents {
        align-self: start;
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    @media (min-width: 1180px){
      d-contents {
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    d-contents nav h3 {
      margin-top: 0;
      margin-bottom: 1em;
    }

    d-contents nav a {
      color: rgba(0, 0, 0, 0.8);
      border-bottom: none;
      text-decoration: none;
    }

    d-contents li {
      list-style-type: none;
    }

    d-contents ul {
      padding-left: 1em;
    }
    
    d-contents nav ul li {
      margin-bottom: .25em;
    }

    d-contents nav a:hover {
      text-decoration: underline solid rgba(0, 0, 0, 0.6);
    }

    d-contents nav ul {
      margin-top: 0;
      margin-bottom: 6px;
    }


    d-contents nav>div {
      display: block;
      outline: none;
      margin-bottom: 0.5em;
    }

    d-contents nav>div>a {
      font-size: 13px;
      font-weight: 600;
    }

    d-contents nav>div>a:hover,
    d-contents nav>ul>li>a:hover {
        text-decoration: none;
    }

    /* code blocks to margins */
    @media (min-width: 1600px) {
      d-code {
        margin-top: -10px;
        grid-column-start: 12;
        grid-column-end: 14; 
      }
    }
    /* so title is on one line */
    d-title h1, d-title p {
      grid-column: middle;
    }

  </style>
  <script>
  // hack to edit font size in code snippets. guaranteed a better way to do 
  // this, but I'm not a webdev
  window.onload = function() {
    setTimeout(() => { document.querySelectorAll("d-code").forEach(function(e) {e.shadowRoot.querySelector('#code-container').style.fontSize = "0.7em"}); }, 3000);
  }
  </script>
  <d-title>
    <h1>Self-Organising Textures</h1>
    <p>Neural Cellular Automata Model of Pattern Formation</p>

  
<svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
    <symbol id="playIcon" viewBox="0 0 24 24"><path d="M8 5v14l11-7z"></path><path d="M0 0h24v24H0z" fill="none"></path></symbol>
    <symbol id="pauseIcon" viewBox="0 0 24 24"><path d="M6 19h4V5H6v14zm8-14v14h4V5h-4z"></path><path d="M0 0h24v24H0z" fill="none"></path></symbol>
    <symbol id="resetIcon" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M12 5V1L7 6l5 5V7c3.31 0 6 2.69 6 6s-2.69 6-6 6-6-2.69-6-6H4c0 4.42 3.58 8 8 8s8-3.58 8-8-3.58-8-8-8z"></path></symbol>
</svg>

<style>
#demo {
    font-size: 14px;
    user-select: none;
    grid-template-columns: auto;
    grid-template-rows: auto auto auto;
    grid-auto-flow: column;
    row-gap: 10px;
}

.hint a {
  color: inherit;
}

@media (min-width: 1000px) {
  #demo {
    grid-template-columns: 1fr 300px;
    grid-template-rows: auto auto;
  }
  #demo-controls {
    grid-row: 1/3;
  }
}

#demo-canvas {
    border: 1px solid lightgrey;
    image-rendering: pixelated;
    touch-action: none;
    width: 512px;
}

#demo-controls {
    line-height: 1em;
    display: grid;
    grid-template-columns: 120px auto;
    grid-template-rows: auto 60px 80px 75px 1fr;
    row-gap: 20px;
    overflow: hidden;
}

@media (min-width: 1000px){
  #demo-controls {
    grid-template-rows: auto 60px 80px 100px 1fr;
  }
}

#pattern-selector {
    grid-column: 1/3;
    display: grid;
    grid-template-columns: repeat(5, auto);
    justify-items: center;
}
@media (max-width: 1000px) and  (min-width: 500px) {
  #pattern-selector {
    grid-template-columns: repeat(10, auto);
  }
}

#pattern-selector * {
    width: 40px;
    height: 40px;
    /* background-image: url('images/emoji.png'); */
    cursor: pointer;
}
.icon {
    width: 30px; height: 30px;
    background: steelblue;
    fill: white;
    border-radius: 20px;
    padding: 5px;
    margin: 2px;
    cursor: pointer;
}
#model-selector {
    line-height: 1.4em;
}
#demo-tip{
    display: grid;
    grid-template-columns: 40px auto;
    align-items: center;
    column-gap: 10px;
    margin-bottom: 20px;
}
#pointer {
    width: 40px;
}
#status {
    font-size: 12px;
    color: rgba(0, 0, 0, 0.6);
    font-family: monospace;
}
#model-hints {
    color: rgba(0, 0, 0, 0.6);
    grid-column: 1/3;
}
#model-hints span {
    display: none;
}
.hint {
    color: rgba(0, 0, 0, 0.6);
    line-height: 1.4em;
    user-select: text;
}

input[type=range] {
  -webkit-appearance: none; /* Hides the slider so that custom slider can be made */
  width: 95%; /* Specific width is required for Firefox. */
  background: transparent; /* Otherwise white in Chrome */
  margin-bottom: 8px;
}

.hint a {
  font-size: 90%;
}

@media (max-width: 350px) {
  .hint a {
    font-size: 75%;
  }
}

input[type=range]::-webkit-slider-thumb {
  -webkit-appearance: none;
}

input[type=range]:focus {
  outline: none; /* Removes the blue border. You should probably do some kind of focus styling for accessibility reasons though. */
}

input[type=range]::-ms-track {
  width: 100%;
  cursor: pointer;

  /* Hides the slider so custom styles can be added */
  background: transparent;
  border-color: transparent;
  color: transparent;
}

/* Thumb */

/* Special styling for WebKit/Blink */
input[type=range]::-webkit-slider-thumb {
  -webkit-appearance: none;
  height: 14px;
  width: 14px;
  border-radius: 50%;
  background: steelblue;
  cursor: pointer;
  margin-top: -6px; /* You need to specify a margin in Chrome, but in Firefox and IE it is automatic */
}

/* All the same stuff for Firefox */
input[type=range]::-moz-range-thumb {
  height: 14px;
  width: 14px;
  border-radius: 50%;
  background: steelblue;
  cursor: pointer;
  border: none;
}

/* All the same stuff for IE */
input[type=range]::-ms-thumb {
  height: 14px;
  width: 14px;
  border-radius: 50%;
  background: grey;
  cursor: pointer;
}

/* Track */

input[type=range]::-webkit-slider-runnable-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}

input[type=range]:focus::-webkit-slider-runnable-track {
  background: rgba(0, 0, 0, 0.15);
}

input[type=range]::-moz-range-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}

input[type=range]::-ms-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}
input[type=range]::-ms-fill-lower {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]:focus::-ms-fill-lower {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]::-ms-fill-upper {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]:focus::-ms-fill-upper {
  background: rgba(0, 0, 0, 0.1);
}

input[type="radio"] {
    background-color: steelblue;
}

#colab-hero-div { 
  grid-column: 1/3;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-top-width: 1px;
  border-top-style: solid;
  border-top-color: rgba(0, 0, 0, 0.1);
  padding-top: 15px;
}

#colab-hero {
  margin: auto;
  display: block;
  text-align: center;
  width: 200px;
  height: 16px;
}

</style>

<div class="l-body-outset grid" id="demo">

    <canvas id="demo-canvas" width="512" height="512"></canvas>

    <div id="demo-tip">
            <img id="pointer" src="images/pointer.svg">
            <div class="hint">
                Click or tap the image to erase the part of the pattern and see it regenerate.
                Double clicking places a new seed cell on the grid.                
            </div>
    </div>

    <div id="demo-controls">
        <div id="pattern-selector">
        </div>
        <div>
            <span id="play-pause">
                <svg class="icon" id="play"><use xlink:href="#playIcon"></use></svg>
                <svg class="icon" id="pause" style="display: none;"><use xlink:href="#pauseIcon"></use></svg>
            </span>
            <svg class="icon" id="reset"><use xlink:href="#resetIcon"></use></svg>
        </div>
        <div>
            Speed: <span id="speedLabel"></span><br>
            <input type="range" id="speed" min="-3" max="3" step="1" value="0"><br>
            <div id="status">
                Step <span id="stepCount"></span>
                (<span id="ips"></span> step/s)
            </div>
        </div>
        <div id="model-selector">
            <!-- Model type:<br>
            <input type="radio" name="model" id="ex1"> <label for="ex1">Growing</label><br>
            <input type="radio" name="model" id="ex2"> <label for="ex2">Persistent</label><br>
            <input type="radio" name="model" id="ex3"> <label for="ex3">Regenerating</label><br>         -->
        </div>
        <div style="white-space: nowrap;">
            Rotation <span id="rotationLabel"></span>&nbsp;<span class="hint"><a href="#experiment-4">[experiment 4]</a></span>
            <br>
            <input type="range" id="rotation" min="0" max="360" step="1" value="0"><br>
        </div>
        <div id='model-hints' class="hint">
            <!-- <span id="ex1-hint">
                <b>Growing</b> models were trained to generate patterns, 
                but don't know how to persist them. Some patterns explode, some decay,
                but some happen to be almost stable or even regenerate parts!
                <a href="#experiment-1">[experiment 1]</a>
            </span>
            <span id="ex2-hint">
                <b>Persistent</b> models are trained to make the pattern stay for a prolonged
                period of time. Interstingly, they often develop some regenerative
                capabilities without being explicitly instructed to do so
                <a href="#experiment-2">[experiment 2]</a>.
            </span>
            <span id="ex3-hint">
                <b>Regenerating</b> models were subject to pattern damages during
                training, so their regenerative capabilities are much stronger,
                especially in the central area. <a href="#experiment-3">[experiment 3]</a>

            </span> -->
        </div>
        <div id="colab-hero-div">
          <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a>
        </div>
    </div>
</div>

<script src="twgl.min.js"></script>
<script src="dat.gui.min.js"></script>
<script type="module">
    import { createDemo } from './demo.js'
    createDemo('demo', ['models.json', 'models_eyvind.json']);
</script>

</d-title>

<d-byline></d-byline>


<d-article>
<d-contents>
  <!--<nav class="l-text toc figcaption">
    <h3>Contents</h3>
    <div><a href="#model">Model</a></div>
    <div><a href="#experiment-1">Experiments</a></div>
    <ul>
      <li><a href="#experiment-1">Learning to Grow</a></li>
      <li><a href="#experiment-2">What persists, exists</a></li>
      <li><a href="#experiment-3">Learning to regenerate</a></li>
      <li><a href="#experiment-4">Rotating the perceptive field</a></li>
    </ul>
    <div><a href="#related-work">Related Work</a></div>
    <div><a href="#discussion">Discussion</a></div>
  </nav>-->
</d-contents>
<h2 id='experiment-ideas'>Experiment ideas:</h2>
<ul><li>A way to pre-train without an objective (self-supervised/unsupervised)</li></ul>
<ul><li>Loss that encourages </li></ul>
<ul><li>3D CAs that get observed by an animal from some perspective - have them learn patterns that work in 2D projection of 3D cell. </li></ul>
<ul><li>Can they count? Try a loss function that “counts” the number of objects in the visualization. </li></ul>
<ul><li>Can we translate between two types of CAs. </li></ul>
<ul><li></li></ul>
<ul><li>Can they compute physics? Try a loss function that enforces some sort of physical phenomenon. If this homogenous compute works - can we convert this to a kernel in physics?</li></ul>

<h2 id='patterns-and'>Patterns and Textures</h2>
<p></p>
<p>Zebra stripes have a characteristic pattern anyone can recognize and describe. At the same time it is said that no two zebras have identical stripes. One would then draw the conclusion that evolution has programmed these cells to produce a certain pattern as opposed to programming them with an exact bitmap of stripes to mould the surface of the Zebra’s body.</p>

<p>We’ve demonstrated the ability for the CA model to almost perfectly recreate any given image using a pixel based loss. Suppose we instead want them to simply approximate a certain “style” or a pattern, and leave them a degree of freedom in terms of the exact pixel values and distribution used to approximate such a pattern. As luck would have it, there’s been extensive research on style transfer and feature visualization by constructing input images to excite or constrain certain intermediate layers of neural networks trained on image tasks, with the go-to network being VGG. Much of this work is summarized and discussed in <u>Differentiable Parametrizations</u>.</p>

<p>Just as in Differentiable Parametrizations, we can employ the building-block aspect of differentiable models to plug in our Cellular Automata as the parametrization layer for an image we are trying to constrain to a certain style - having the CAs act as the “renderer”, so to speak, and having a second network, such as VGG, act as a distinguisher of patterns - providing the gradient necessary to produce a pattern of a certain style</p>

<p>Let’s give this simple experiment a try - processing a pattern of interest through VGG, we pick an intermediate layer which is known to be a good representation of the “styles” involved and note the activations of the neurons. Now, we connect the output of our Cellular Automata into VGG and backpropagate into the parameters of the automata</p>
<p>Immediately notice that they produce coherent patterns in the style of our requested input image, and are able to learn these in very few steps.</p>

<p>[CLICKABLE INTERACTIVE IMAGE GRAPHIC]</p>

<p>What’s more interesting and not readily apparent in snapshots of batches is that the patterns are in fact not static. Taking a look at the video below, we see all manners of gradual motion. This is not entirely unexpected - in the definition of our experiments we do not place any constraints which individual pixel values should be - instead we just want the overall output to approximate a pattern we chose through our target image. Given the many convolutional layers in VGG, the network likely does not depend much on the exact spatial placement of the pattern, but rather only with its presence.</p>

<h2 id='feature-visualization'>Feature Visualization</h2>

<p>Some (all?) butterflies have remarkably lifelike eyes on their wings. Evolution has placed these there to trigger a response of fear in potential predators. It is likely that neither the predator nor the butterfly has a concept for what an eye is or does, or even less so any <u>theory of mind</u> regarding each other, but the magic of evolutionary development has nonetheless reached a state where the butterfly ingeniously tricks predators into fearing a harmless bug instead of consuming it. </p>

<p>Even more remarkable is the fact that the individual cells composing the butterfly’s wings can self assemble into coherent, beautiful, shapes far larger than an individual cell - indeed a cell is on the order of 1e-...m while the features on the wings will grow to as large as 1e-...m. Communication over this distance implies a self-organization over a distance of millions of cells.</p>

<p>In addition to style transfer -  a common approach to investigating neural networks (cite: OpenAI microscope) is to look at what inhibits or excites individual neurons in a network. Just as neuroscientists and biologists have often treated cells and cell structures and neurons as black-box models to be investigated, measured and reverse-engineered, there’s a large contemporary body of work on doing the same with neural networks. </p>

<p>Luckily, we can explore this idea with minimal effort by simply taking our VGG implementation from before and exploring what happens if we are to generate a CA that excites a given neuron. </p>

<p>[EYE GENERATING CA GRAPHIC]</p>
<h2 id='hidden-states'>Hidden states</h2>
<p>There are various ways to visualize these communication channels - but one of them is to add to cells a potential-activated dye - allowing a clear picture of the voltage potential the cell is under with respect to the surrounding substrate. This technique has provided useful insight into the communication patterns within groups of cells - showing both local and nonlocal communication over a variety of time-scales.</p>

<p>As luck would have it - we can do something very similar with our Cellular Automata. Recall that our CA model consists of three visible channels, with the rest being treated as latent channels visible to the update step but excluded from any loss function. . The figure below visualizes the magnitudes of these hidden channels when mapped to intensity in RGB. </p>

<p>We immediately notices several interesting aspects.</p>

<p>This means the hidden channels can be considered to be “floating” (to borrow a term from circuits) - they are not being pulled to any specific final state or intermediate state and instead converge to some dynamical system assisting the cell fulfill its “true” objective</p>

<p>The flow of gradient back through the CA enters the CA at its final iteration. This suggests that final dynamics of the hidden state must be strongly influenced by the first gradient update it receives “through” this final iteration. Indeed we can consider this to be a dynamical system within a dynamical system. CA back through the preceding iterations gives us a clue as to what aspects of the CA’s training may influence what the hidden state dynamical system looks like. </p>

<p>When biological cells communicate with each other, they do so through a multitude of available communication channels. Cells can emit or absorb different ions and proteins, sense physical motion or “stiffness” of other cells or even emit different chemical signals to diffuse over the local substrate (citation needed + cleanup for all these). </p>


<p>Visualization </p>
<ul><li>Hidden state analysis yields some interesting results. For instance - certain anomalies in patterns will only appear after some time (eyes) and you can see distinct activity in the regions where they are about to appear. </li></ul>
<ul><li>Hidden states for automata trained from the same ancestors have similar magnitudes.</li></ul>
<ul><li>Visualization concept:</li></ul>
<ul><li>Show hidden state alongside the the RGB state, and highlight the neural activity in the hidden state with a red circle or “highlight” effect (dark overlay + lightened focus)   </li></ul>
<h2 id='finetuning-cellular'>Fine-tuning Cellular Automata</h2>
<p>One could theorize that a large part of the training process for our CAs consists of. </p>

<ul><li>Initializing with existing checkpoint dramatically accelerates convergence. Let’s make some </li></ul>
<ul><li>Leads to a better CA</li></ul>
<h2 id='cooperating-relatives'>Cooperating Relatives</h2>

<ul><li>CAs trained starting from the same checkpoint (“relatives”) can interface with each other. </li></ul>
<ul><li>Experiment interpolation between the parameters of these two </li></ul>



</d-article>
<d-appendix>

<d-footnote-list></d-footnote-list>
<d-citation-list></d-citation-list>
<d-appendix>
  <d-bibliography src="bibliography.bib"></d-bibliography>
</body>
